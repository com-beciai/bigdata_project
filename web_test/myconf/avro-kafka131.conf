#第二层agent，部署到mini2和mini3

# flume-ng agent -n a1 -c /usr/soft/flume/conf -f /usr/soft/flume/myconf/avro-kafka131.conf  -Dflume.root.logger=INFO,console
# nohup flume-ng agent -n a1 -c /usr/soft/flume/conf -f /usr/soft/flume/myconf/avro-kafka131.conf 1>/dev/null 2>/dev/null &
a1.sources = r1
a1.channels = c1
a1.sinks = k1

#source
a1.sources.r1.type = avro
a1.sources.r1.channels = c1
#这个需要修改
a1.sources.r1.bind = 192.168.88.131
a1.sources.r1.port = 9999

#channel
a1.channels.c1.type = file
#这个需要修改
a1.channels.c1.checkpointDir = /usr/soft/flume/checkpoint/channel
#这个需要修改
a1.channels.c1.dataDirs =  /usr/soft/flume/mydata/channels
a1.channels.c1.maxFileSize = 104857600
a1.channels.c1.capacity = 90000000
a1.channels.c1.keep-alive = 60


#sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.topic = event_log
#这个需要修改（kafka节点地址）
a1.sinks.k1.brokerList = 1804-LJF:9092,1804-LJF02:9092,1804-LJF03:9092

#保证消息完整生成
# 设置发送数据是否需要服务端的反馈,有四个值0,1,-1 all
# 0: producer不会等待broker发送ack
# 1: 当leader接收到消息之后发送ack
# -1: 当所有的follower都同步消息成功后发送ack.
a1.sinks.k1.requiredAcks = 1
a1.sinks.k1.kafka.producer.type = sync
a1.sinks.k1.batchSize = 1
a1.sinks.k1.channel = c1

#a1.sinks.k1.type = logger
#a1.sinks.k1.channel = c1
